{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd0eH4JxE2+JqHrJKcJpOC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreenidhikamath/book_scraping/blob/main/bookscraping_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QzMOQPrGo9Q"
      },
      "outputs": [],
      "source": [
        "#importing necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the base url\n",
        "BASE_URL = 'http://books.toscrape.com/catalogue/'\n",
        "#Here we extract only the text content of the first page of website\n",
        "START_URL = 'http://books.toscrape.com/catalogue/page-1.html'"
      ],
      "metadata": {
        "id": "KZf7jgfsGr2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is a function written to obtain the rating(numerical value) from the categorical value i.e 1 from 'One' for example\n",
        "#here a dictionary rating is taken to show the key value pairs of numerical and categorical values\n",
        "def rating(star_class):\n",
        "    ratings = {\"One\": 1,\"Two\": 2,\"Three\": 3,\"Four\": 4,\"Five\": 5}\n",
        "    for i,j in ratings.items():\n",
        "        if i in star_class:\n",
        "            return j\n",
        "    return None"
      ],
      "metadata": {
        "id": "f5ZAlDpjGr59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the most important function in the program which helps to obtain information of every book\n",
        "def scrape():\n",
        "    # a list books is taken to put the necessary attributes as user requirements\n",
        "    books = []\n",
        "    # we begin with the first page of the base url\n",
        "    page_number = 1\n",
        "    # A loop is designed to find information of all 1000 books as required in the user story\n",
        "    while len(books) < 1000:\n",
        "        url = f\"{BASE_URL}page-{page_number}.html\"\n",
        "        #to obtain response from the website we can use requests.get from the request module\n",
        "        #sometimes some websites do not provide scraping oppurtunity to keep away from bot access so an additional headers line will be required but here it works without it.\n",
        "        response = requests.get(url)\n",
        "        #if there is no response from the website then it provides status codes from 400 generally if the website is ok then 200 is given\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve page {page_number}\")\n",
        "            break\n",
        "        #now let us extract the html type content of webpage\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        #under the article section inside the class product_pod we have the necessary book information to extract so let us navigate to it using soup.select\n",
        "        book_list = soup.select('article.product_pod')\n",
        "        #if there are no more books means we have come out from the class product_pod of article section so we will print no more books and come out of loop.\n",
        "        if not book_list:\n",
        "            print(\"No more books found.\")\n",
        "            break\n",
        "        #now we navigate through items of the list and add each item\n",
        "        for book in book_list:\n",
        "            #the book title is a link and a h3 type and it is present inside title section\n",
        "            title = book.h3.a['title']\n",
        "            #select_one a function of soup returns the first matching tag as a tag object or returns none if not found\n",
        "            #here price is a paragraph type and is present in price_color class so only the text part is extracted and not the currency symbol using text.strip()\n",
        "            price = book.select_one('p.price_color').text.strip()\n",
        "            #here the rating is accessed by star-rating class\n",
        "            rating_class = book.select_one('p.star-rating')['class']\n",
        "            #function rating is called\n",
        "            rate = rating(rating_class)\n",
        "            #availability is accessed inside instock availability class and only the text is extracted\n",
        "            availability = book.select_one('p.instock.availability').text.strip()\n",
        "            #the product url is a h3 type but there are various h3 elements so h3.a refers to all link elements and we require the href portion of it\n",
        "            product_relative_url = book.h3.a['href']\n",
        "            #each product_url are partially present so it must be joined with the base url which is done using urljoin fuction\n",
        "            product_url = urljoin(BASE_URL, product_relative_url)\n",
        "            #now all the attributes are appended to the books list as a key-pair value where the keys represent the name of attributes and value represents their actual value\n",
        "            books.append({\n",
        "                'Title': title,\n",
        "                'Price': price,\n",
        "                'Rating': rate,\n",
        "                'Availability': 'In stock' if 'In stock' in availability else 'Out of stock',\n",
        "                'Product URL': product_url\n",
        "            })\n",
        "        #we navigate every page so page number is increemented\n",
        "        page_number +=1\n",
        "\n",
        "    return books"
      ],
      "metadata": {
        "id": "YA9Yu1K4Gr8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape and save to CSV\n",
        "book_data = scrape()\n",
        "df = pd.DataFrame(book_data)\n",
        "#remove the index assigned by to_csv operation by default\n",
        "df.to_csv('books_data.csv', index=False)"
      ],
      "metadata": {
        "id": "g060eV-ZGr__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)  #sends a GET request to the given URL\n",
        "        response.raise_for_status()               #raises error if the response has a bad status (like 404)\n",
        "        return response                           #returns the response object if everything's okay\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")        #prints the error if request fails\n",
        "        return None                                #returns None if there was any issue"
      ],
      "metadata": {
        "id": "w3DqJyD1GsC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "#sets up logging to save errors in a file instead of stopping the script\n",
        "logging.basicConfig(filename=\"scraping_errors.log\", level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "def process_books(book_list, books):\n",
        "    for book in book_list:  #go through each book in the list\n",
        "        try:\n",
        "            #get the main parts of the book info\n",
        "            title_tag = book.h3.a\n",
        "            price_tag = book.select_one('p.price_color')\n",
        "            rating_tag = book.select_one('p.star-rating')\n",
        "            availability_tag = book.select_one('p.instock.availability')\n",
        "            product_url_tag = book.h3.a\n",
        "\n",
        "            #check if any important data is missing\n",
        "            if None in [title_tag, price_tag, rating_tag, availability_tag, product_url_tag]:\n",
        "                raise ValueError(\"Missing Data\")  #throw error if something's missing\n",
        "\n",
        "            #clean and store the data properly\n",
        "            title = title_tag['title']\n",
        "            price = price_tag.text.strip().replace(\"Â£\", \"\").replace(\"£\", \"\")\n",
        "            rating_class = rating_tag['class'][1]  #gets the rating word like 'Three'\n",
        "            availability = availability_tag.text.strip()\n",
        "            product_url = product_url_tag['href']\n",
        "\n",
        "            #add all the data to the books list\n",
        "            books.append({\n",
        "                'Title': title,\n",
        "                'Price': float(price),\n",
        "                'Rating': rating_class,\n",
        "                'Availability': 'In stock' if 'In stock' in availability else 'Out of stock',\n",
        "                'Product URL': product_url\n",
        "            })\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Skipping book due to missing data: {e}\")  #log error if something goes wrong\n"
      ],
      "metadata": {
        "id": "2Q31RzgQGsF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape():\n",
        "    #start with an empty list to store the book details\n",
        "    books = []\n",
        "    page_number = 1  #begin from the first page of the catalog\n",
        "\n",
        "    #keep going until we collect about 1000 books\n",
        "    while len(books) < 1000:\n",
        "        #construct the URL for the current page\n",
        "        url = f\"http://books.toscrape.com/catalogue/page-{page_number}.html\"\n",
        "\n",
        "        #try to fetch the page content using a safe method\n",
        "        response = fetch_page(url)\n",
        "\n",
        "        #if the page couldn't be loaded, stop the scraping process\n",
        "        if response is None:\n",
        "            break\n",
        "\n",
        "        #parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        #find all the book entries on this page\n",
        "        book_list = soup.select('article.product_pod')\n",
        "\n",
        "        #if there are no books on the page, we've likely reached the end\n",
        "        if not book_list:\n",
        "            break\n",
        "\n",
        "        #extract the details we care about from each book and add them to our list\n",
        "        process_books(book_list, books)\n",
        "\n",
        "        #move on to the next page\n",
        "        page_number += 1\n",
        "\n",
        "    #once done, return the complete list of books we've gathered\n",
        "    return books\n"
      ],
      "metadata": {
        "id": "vtXYItlWGsI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#turn the list of book data into a DataFrame for easier handling and analysis\n",
        "df = pd.DataFrame(scrape())\n",
        "\n",
        "#check if we actually got any data\n",
        "if not df.empty:\n",
        "    #save the data to a CSV file, without the row index\n",
        "    df.to_csv('books_data.csv', index=False)\n",
        "    print(\"Data saved successfully.\")\n",
        "else:\n",
        "    #let the user know that something went wrong or no data was found\n",
        "    print(\"No data found. Check logs for errors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D8wXaLmGsLw",
        "outputId": "7ef41029-ca7b-4c2a-bf29-b8decc028611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"books_data.csv exists:\", os.path.isfile(\"books_data.csv\"))  #check if the CSV file is already there"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7ltA4fAGsPK",
        "outputId": "a2512453-6c51-477d-9ed5-f6d6121aa655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "books_data.csv exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"books_data.csv\")\n",
        "print(df.head())  #show the first few rows to see what’s inside"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRJzTQHZGsSx",
        "outputId": "75eee19c-76f8-4086-b000-5826794f2ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   Title  Price Rating Availability  \\\n",
            "0                   A Light in the Attic  51.77  Three     In stock   \n",
            "1                     Tipping the Velvet  53.74    One     In stock   \n",
            "2                             Soumission  50.10    One     In stock   \n",
            "3                          Sharp Objects  47.82   Four     In stock   \n",
            "4  Sapiens: A Brief History of Humankind  54.23   Five     In stock   \n",
            "\n",
            "                                         Product URL  \n",
            "0               a-light-in-the-attic_1000/index.html  \n",
            "1                  tipping-the-velvet_999/index.html  \n",
            "2                          soumission_998/index.html  \n",
            "3                       sharp-objects_997/index.html  \n",
            "4  sapiens-a-brief-history-of-humankind_996/index...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"File extension:\", os.path.splitext(\"books_data.csv\")[1])  #just checking the file type here\n",
        "print(df[\"Price\"].dtype)  #see what kind of data type the Price column has"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgLgtUckGsWK",
        "outputId": "5dd19ca6-4ed5-4331-ce20-41415fb32d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File extension: .csv\n",
            "float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expected_columns = [\"Title\", \"Price\", \"Rating\", \"Availability\", \"Product URL\"]\n",
        "print(df.columns.tolist() == expected_columns)  #check if the columns match what we expect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZvIk1mLHT_j",
        "outputId": "18c9a7e3-2032-40b5-a0a7-aadec0e024f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())  #see if any data is missing in the columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MC1ZC4fHUCq",
        "outputId": "991d532a-3a04-4ba3-e48d-bbe3d4ff1ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title           0\n",
            "Price           0\n",
            "Rating          0\n",
            "Availability    0\n",
            "Product URL     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#see if the log file is there before opening it\n",
        "if os.path.isfile(\"scraping_errors.log\"):\n",
        "    with open(\"scraping_errors.log\", \"r\") as file:\n",
        "        errors = file.readlines()\n",
        "        print(\"Errors logged:\", len(errors) > 0)  #true if any errors got saved\n",
        "else:\n",
        "    print(\"Errors logged: False (log file not found, so no errors logged)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgTypVnyHUGO",
        "outputId": "22a43f63-b669-41e1-b35d-c60d3374da8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errors logged: False (log file not found, so no errors logged)\n"
          ]
        }
      ]
    }
  ]
}